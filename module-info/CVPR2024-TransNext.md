# TransNeXt核心模块详解 https://arxiv.org/pdf/2311.17132

## 一、Aggregated Attention（聚合注意力）

### 1. 背景

#### 现有问题
- **深度退化效应**：许多高效ViT模型依赖堆叠层进行信息交换，但由于残差连接中的深度退化效应，无法形成充分的信息混合[1]
- **与生物视觉的差异**：现有的局部注意力和空间下采样注意力与生物视觉系统工作原理存在显著差异[3]
- **窗口分割artifacts**：基于窗口分割的方法会产生不自然的块状痕迹，即使经过深层堆叠也无法消除[3]
- **计算复杂度**：全局自注意力的二次复杂度限制了在高分辨率图像上的应用[1]

#### 生物视觉启发
人类视觉系统具有中央凹视觉（高敏锐度，覆盖1-2度视野）和周边视觉（大感受野但精度较低）的二分法特性。眼球通过快速运动（扫视）处理多个视野信息并进行整合[20]。

### 2. 模块原理

#### 核心设计：像素聚焦注意力（Pixel-focused Attention）
采用**双路径设计**模拟生物视觉系统：

**路径1：滑动窗口注意力**（模拟中央凹视觉）
- 每个查询对其最近邻特征进行细粒度感知
- 使用固定的k×k窗口（实验中采用3×3）[5][6]

**路径2：池化注意力**（模拟周边视觉）  
- 每个查询对空间下采样特征进行粗粒度全局感知
- 通过"激活和池化"操作获得全局信息[6]

**数学表达**：
```
S(i,j)~ρ(i,j) = Q(i,j)K^T_ρ(i,j)     # 滑动窗口路径
S(i,j)~σ(X) = Q(i,j)K^T_σ(X)         # 池化路径
A(i,j) = softmax(Concat(S(i,j)~ρ(i,j), S(i,j)~σ(X))/√d + B(i,j))
```

#### 增强机制
1. **查询嵌入（Query Embedding）**：添加可学习的查询令牌，增强注意力矩阵生成的多样性[7]
2. **位置注意力（Positional Attention）**：使用可学习令牌与查询交互，提供动态相对位置偏置[8]
3. **长度缩放余弦注意力**：提升多尺度输入的外推能力，λ = τ log N[9]

### 3. 解决的问题

1. **避免深度退化**：不依赖堆叠进行信息交换，单层即可实现有效的局部-全局建模[1]
2. **自然视觉感知**：消除窗口分割产生的不自然块状artifacts，实现更符合生物视觉的感知模式[3]
3. **像素级平移等变性**：模拟眼球连续运动，对图像任意位置的像素都能提供一致的中央凹视觉特性[3]
4. **线性复杂度**：当池化大小固定时，计算复杂度与输入序列长度呈线性关系[10]
5. **多尺度适应**：通过长度缩放余弦注意力和log-CPB位置偏置，提升大尺度图像的外推性能[9]

---

## 二、Convolutional GLU（卷积GLU）

### 1. 背景

#### ViT时代的通道注意力需求
- **SE机制的局限性**：在ViT时代，全局感受野不再稀缺，SE机制使用全局平均池化的方法显得过于粗粒度，所有令牌共享相同的门控信号[11]
- **ViT缺乏通道注意力**：研究发现将SE机制引入通道混合器可以有效增强模型鲁棒性[11]
- **位置信息需求**：ViT结构需要通过3×3深度卷积提供条件位置编码（CPE）[11]

#### GLU的优势
门控线性单元（GLU）在自然语言处理任务中表现优于MLP，由两个线性投影组成，其中一个通过门控函数激活[11]。

### 2. 模块原理

#### 设计理念
将**最小形式的3×3深度卷积**添加到GLU门控分支的激活函数之前，使其符合门控通道注意力的设计理念[11]。

#### 结构设计
```
ConvGLU(X) = (XW1 + B1) ⊙ GELU(DWConv(XW2 + B2))
```

其中：
- `XW1 + B1`：值分支（保持与MLP和GLU相同的深度）
- `DWConv(XW2 + B2)`：门控分支（添加3×3深度卷积）
- `⊙`：逐元素乘法
- `GELU`：激活函数

#### 关键特性
1. **基于最近邻特征的门控**：每个令牌拥有基于其最近邻细粒度特征的独特门控信号[12]
2. **反向传播友好**：值分支保持与MLP相同的深度[12]
3. **计算效率**：相比ConvFFN，在保持相同参数量的情况下，FLOPs更少[12]

### 3. 解决的问题

1. **细粒度通道注意力**：解决SE机制过于粗粒度的问题，每个令牌都有独特的门控信号[12]
2. **位置信息编码**：为没有位置编码设计的ViT模型提供必要的位置信息[11]
3. **增强鲁棒性**：通过基于局部特征的通道注意力机制，有效提升模型鲁棒性[11]
4. **计算效率优化**：实现注意力化的通道混合器，同时减少计算开销[12]
5. **满足ViT多样化需求**：简单而鲁棒的设计满足ViT的各种需求[12]

#### 消融实验验证
在CIFAR-100上的实验表明，ConvGLU相比其他变体（Type-1、Type-2、Type-3）表现最佳，验证了将深度卷积放在门控分支激活函数前的设计合理性[27]。

---

## 总结

Aggregated Attention和Convolutional GLU分别作为令牌混合器和通道混合器，共同构成了TransNeXt的核心。前者通过仿生视觉设计解决了深度退化和不自然视觉感知问题，后者通过改进的门控机制提升了通道建模能力和鲁棒性。两个模块的结合使TransNeXt在各种视觉任务上达到了最先进的性能[1][19]。